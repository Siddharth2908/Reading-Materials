{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cda2204febb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import string\n",
    "import datetime\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, roc_auc_score, log_loss, precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Setup environment\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "# present directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# portal directory\n",
    "portal_dir = '/apps/rpauser/portal/rpaportal/app/static/src/assets/ml/'\n",
    "#portal_dir = '/tmp/'\n",
    "\n",
    "# Class for dumping o/p as json\n",
    "class model_output(object):\n",
    "  def __init__(self, Directory, Date, Id, Model, AUC, Accuracy, LogLoss, Precision, Input, Prediction, Details):\n",
    "    self.Directory = cwd + '/' + Directory\n",
    "    self.Date = str(Date)\n",
    "    self.Id = Id\n",
    "    self.Model = Model\n",
    "    self.Accuracy = Accuracy\n",
    "    self.AUC = AUC\n",
    "    self.LogLoss = LogLoss\n",
    "    self.Precision = Precision\n",
    "    self.Input = Input\n",
    "    self.Prediction = Prediction\n",
    "    self.Details = Details\n",
    "\n",
    "  def set_value(self, name, value):\n",
    "    if name == \"AUC\":\n",
    "      self.AUC = str(\"%0.2f\" % value)\n",
    "    elif name == \"Accuracy\":\n",
    "      self.Accuracy = str((\"%0.2f\" % value) * 100) + '%'\n",
    "    elif name == \"LogLoss\":\n",
    "      self.LogLoss = str(\"%0.2f\" % value)\n",
    "    elif name == \"Precision\":\n",
    "      self.Precision = str((\"%0.2f\" % value) * 100) + '%'\n",
    "    elif name == \"Input\":\n",
    "      self.Input = value\n",
    "    elif name == \"Prediction\":\n",
    "      self.Prediction = value\n",
    "    elif name == \"Details\":\n",
    "      self.Details = value\n",
    "    else:\n",
    "      print('output value not supported.')\n",
    "\n",
    "\n",
    "  def dump_json(self):\n",
    "   f = open(self.Directory + '/model_output.json', 'w')\n",
    "   f.write(json.dumps(self, default=jdefault, indent=4, sort_keys=True))\n",
    "   f.close()\n",
    "\n",
    "\n",
    "# Utility for json dumping\n",
    "def jdefault(o):\n",
    "  if isinstance(o, set):\n",
    "    return list(o)\n",
    "  return o.__dict__\n",
    "\n",
    "# Print and log into output folder\n",
    "def log(outdir, desc, value):\n",
    "  print(desc)\n",
    "  print(str(value))\n",
    "  filename = outdir + '/log.txt'\n",
    "  f = open(filename, 'a')\n",
    "  f.write(desc)\n",
    "  f.write(str(value))\n",
    "  f.write('\\n')\n",
    "  f.close()\n",
    "  # Copy for portal usage\n",
    "  shutil.copy2(filename, portal_dir + filename.replace(\"/\", \"\"))\n",
    "\n",
    "# Utility function to setup directory to persist output\n",
    "def prep_outdir(id_):\n",
    "   now = datetime.datetime.now()\n",
    "   directory =  os.path.join('output', now.strftime(\"%Y%m%d%H%M%S\")+'_'+id_)\n",
    "   if not os.path.exists(directory):\n",
    "     os.makedirs(directory)\n",
    "   return now, directory\n",
    "\n",
    "# Utility function to save hyperparameters\n",
    "def save_params(now, directory, id_, model):\n",
    "   filename = directory + '/log.txt'\n",
    "   f = open(filename, 'a')\n",
    "   f.write('Time=' + str(now))\n",
    "   f.write('\\nId=' + id_)\n",
    "   f.write('\\nModel=' + model )\n",
    "   f.write('\\nDetails=' + filename.replace(\"/\", \"\"))\n",
    "   f.close()\n",
    "   # Copy for portal usage\n",
    "   shutil.copy2(filename, portal_dir + filename.replace(\"/\", \"\"))\n",
    "\n",
    "   mop = model_output(directory, now, id_, model,  '', '', '', '', '', '', filename.replace(\"/\", \"\"))\n",
    "   mop.dump_json()\n",
    "   return mop\n",
    "\n",
    "# Utility function to save prediction\n",
    "def save_prediction(directory, mop, prediction):\n",
    "   filename = directory + '/prediction.csv'\n",
    "   prediction.to_csv(filename, sep=\",\", encoding='utf-8')\n",
    "   # Copy for portal usage\n",
    "   shutil.copy2(filename, portal_dir + filename.replace(\"/\", \"\"))\n",
    "   mop.set_value('Prediction', filename.replace(\"/\", \"\"))\n",
    "   mop.dump_json()\n",
    "\n",
    "\n",
    "# Convert string to a list\n",
    "def convert_to_list(text):\n",
    "  list_data = []\n",
    "  words = text.split()\n",
    "  for word in words:\n",
    "    list_data.append(word)\n",
    "  return list_data\n",
    "\n",
    "# Remove punctuations, stop words, stemming\n",
    "def clean_text(text):\n",
    "  text = re.sub(r\"\\r\", \"\", text)\n",
    "  text = re.sub(r\"\\n\", \"\", text)\n",
    "  text = text.strip(' ')\n",
    "  return text\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(dataframe):\n",
    "  # Create binary feature\n",
    "  df = dataframe\n",
    "  mapping = {'Acknowledgement': 1, 'Other': 0, 'Partial_Answer': 0, 'Full_Answer' :0}\n",
    "  df = df.replace({'label': mapping})\n",
    "  dataframe[\"ack\"] = df[\"label\"]\n",
    "\n",
    "  df = dataframe\n",
    "  mapping = {'Acknowledgement': 0, 'Other': 1, 'Partial_Answer': 0, 'Full_Answer' :0}\n",
    "  df = df.replace({'label': mapping})\n",
    "  dataframe[\"other\"] = df[\"label\"]\n",
    "\n",
    "  df = dataframe\n",
    "  mapping = {'Acknowledgement': 0, 'Other': 0, 'Partial_Answer': 1, 'Full_Answer' :0}\n",
    "  df = df.replace({'label': mapping})\n",
    "  dataframe[\"partial\"] = df[\"label\"]\n",
    "\n",
    "  df = dataframe\n",
    "  mapping = {'Acknowledgement': 0, 'Other': 0, 'Partial_Answer': 0, 'Full_Answer' :1}\n",
    "  df = df.replace({'label': mapping})\n",
    "  dataframe[\"full\"] = df[\"label\"]\n",
    "\n",
    "  dataframe['msg'] = dataframe['msg'].map(lambda x : clean_text(x))\n",
    "  return dataframe\n",
    "\n",
    "# Read the input data for training and validation\n",
    "def read_input(now, outdir):\n",
    "  # Read input data\n",
    "  df = pd.read_csv(\"input/training.csv\", sep=\",\", encoding='latin-1')\n",
    "\n",
    "  log(outdir, 'Input dataframe before preprocessing', df)\n",
    "  log(outdir, 'Input dataframe describe before preprocessing', df.describe())\n",
    "\n",
    "  # Randomize and cleanup the data\n",
    "  df = df.reindex(np.random.permutation(df.index))\n",
    "  df = preprocess_data(df)\n",
    "\n",
    "  log(outdir, 'Input dataframe after preprocessing', df)\n",
    "  log(outdir, 'Input dataframe describe after preprocessing', df.describe())\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "# Pipeline Factory\n",
    "def get_pipeline(model):\n",
    "  if model == \"MultinomialNB\":\n",
    "    pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "  elif model == \"LinearSVC\":\n",
    "    pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "  elif model == \"LogisticRegression\":\n",
    "    pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "  else:\n",
    "    print('Model not supported')\n",
    "  return pipeline\n",
    "\n",
    "# Params Factory\n",
    "def get_params(model):\n",
    "  if model == \"MultinomialNB\":\n",
    "    params = {\n",
    "      'tfidf__use_idf': (True, False),\n",
    "    }\n",
    "  elif model == \"LinearSVC\":\n",
    "    params = {\n",
    "      'tfidf__use_idf': (True, False),\n",
    "    }\n",
    "  elif model == \"SGDClassifier\":\n",
    "    params = '' \n",
    "  elif model == \"LogisticRegression\":\n",
    "    params = {\n",
    "      'tfidf__use_idf': (True, False),\n",
    "    }\n",
    "  else:\n",
    "    print('Model not supported')\n",
    "  return params\n",
    "\n",
    "# Train the model \n",
    "def train_validate_model(model, mop, df, df_test, now, outdir):\n",
    "\n",
    "  categories = ['ack', 'other', 'partial', 'full']\n",
    "  train, vald = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
    "  X_train = train.msg\n",
    "  X_vald = vald.msg\n",
    "  X_test = df_test.msg\n",
    "  log(outdir, 'Training data size: ', str(X_train.shape[0]))\n",
    "  log(outdir, 'Validate data size: ', str(X_vald.shape[0]))\n",
    "  log(outdir, 'Test data size: ', str(X_test.shape[0]))\n",
    "\n",
    "  # Define a pipeline combining a text feature extractor with multi lable classifier\n",
    "  pipeline = get_pipeline(model)\n",
    "\n",
    "  result = df_test\n",
    "  # Train the model, minimise loss\n",
    "  for category in categories:\n",
    "    log(outdir, '... Processing ...', category)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "      get_pipeline(model),  # pipeline\n",
    "      get_params(model),    # parameters to tune via cross validation\n",
    "      refit=True,           # fit using all available data at the end, on the best found param combination\n",
    "      n_jobs=-1,            # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "      scoring='accuracy',   # what score are we optimizing?\n",
    "      cv=StratifiedKFold(train[category], n_folds=5),  # what type of cross validation to use\n",
    "    )\n",
    "    # train the model \n",
    "    model_detector = grid.fit(X_train, train[category])\n",
    "\n",
    "    # compute the validation accuracy\n",
    "    prediction = model_detector.predict(X_vald)\n",
    "\n",
    "    mop.set_value('Accuracy', accuracy_score(vald[category], prediction))\n",
    "    log(outdir, 'accuracy is: ', accuracy_score(vald[category], prediction))\n",
    "\n",
    "    mop.set_value('AUC', roc_auc_score(vald[category], prediction))\n",
    "    log(outdir, 'roc_auc is: ', roc_auc_score(vald[category], prediction))\n",
    "\n",
    "    mop.set_value('LogLoss', log_loss(vald[category], prediction))\n",
    "    log(outdir, 'log_loss is: ', log_loss(vald[category], prediction))\n",
    "\n",
    "    mop.set_value('Precision', precision_score(vald[category], prediction))\n",
    "    log(outdir, 'precision is: ', precision_score(vald[category], prediction))\n",
    "\n",
    "    mop.dump_json()\n",
    "\n",
    "    log(outdir, 'Confusion Matrix', confusion_matrix(vald[category], prediction))\n",
    "    log(outdir, 'Classification Report', classification_report(vald[category], prediction))\n",
    "\n",
    "    # predict the testing data\n",
    "    prediction = model_detector.predict(X_test)\n",
    "    prediction_ = pd.DataFrame(prediction)\n",
    "    prediction_.columns = prediction_.columns.astype(str)\n",
    "    prediction_.rename(columns={\"0\": category}, inplace=True)\n",
    "    prediction_.reset_index(drop=True, inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "    result = pd.concat([result, prediction_], axis=1)\n",
    "\n",
    "    # store the spam detector to disk after training\n",
    "    filename = outdir + '/' + category + '-model.pkl'\n",
    "    with open(filename, 'wb') as out:\n",
    "      cPickle.dump(model_detector, out)\n",
    "      log(outdir, 'Model Saved', filename)\n",
    "\n",
    "\n",
    "  log(outdir, \"Prediction: \", result)\n",
    "  save_prediction(outdir, mop, result)\n",
    "\n",
    "\n",
    "# Testing of the model\n",
    "def read_test(now, outdir):\n",
    "\n",
    "  # Read input data\n",
    "  df_test = pd.read_csv(\"input/test.csv\", sep=\"\\t\", encoding='latin-1')\n",
    "\n",
    "  # Randomize and cleanup the data\n",
    "  #test = preprocess_data(test)\n",
    "  df_test['msg'] = df_test['msg'].map(lambda x : clean_text(x))\n",
    "\n",
    "  log(outdir, 'Test dataframe describe after preprocessing', df_test)\n",
    "  log(outdir, 'Test dataframe describe after preprocessing', df_test.describe())\n",
    "\n",
    "  return df_test\n",
    "\n",
    "\n",
    "# Training validation  and testing of the model\n",
    "def train_validate_test():\n",
    "\n",
    "  hyperparameters_dataframe = pd.read_csv(\"input/hyperparams.csv\", sep=\",\")\n",
    "  print('Hyperparameters')\n",
    "  print(hyperparameters_dataframe)\n",
    "  for index, row in hyperparameters_dataframe.iterrows():\n",
    "\n",
    "    id_ = row[0]\n",
    "    modl = row[1]\n",
    "\n",
    "    # Read Data, normalize, preprocess features, view\n",
    "    print('\\n\\n ************ Training new model - ' + id_ + ' *************')\n",
    "\n",
    "    # Creates an outputdir for the model\n",
    "    now, outdir = prep_outdir(id_)\n",
    "\n",
    "    # saving all config\n",
    "    mop = save_params(now, outdir, id_, modl)\n",
    "\n",
    "    # Reading input\n",
    "    df = read_input(now, outdir)\n",
    "\n",
    "    # Reading test\n",
    "    df_test = read_test(now, outdir)\n",
    "\n",
    "    # Train Model\n",
    "    train_validate_model(modl, mop, df, df_test, now, outdir)\n",
    "\n",
    "    # Test Model\n",
    "    #test(pipeline, mop, outdir)\n",
    "\n",
    "\n",
    "# Main\n",
    "train_validate_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
